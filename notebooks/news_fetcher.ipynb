# !pip install newspaper3k feedparser beautifulsoup4 requests transformers accelerate sentencepiece diffusers spacy pycountry tqdm --quiet
# !python -m spacy download en_core_web_sm
# !pip install transformers datasets spacy --quiet
# !python -m spacy download en_core_web_sm
# !pip install pycountry
# !pip install 'lxml[html_clean]'


# üß† GeoImpact AI ‚Äì News Fetcher (Donald Trump Focus)
# üìÖ Created for Hugging Face + Colab

# Install required libraries
!pip install newspaper3k feedparser beautifulsoup4 requests --quiet

# Imports
from newspaper import Article
import requests
from bs4 import BeautifulSoup
import feedparser
import json

# =============== REUTERS SCRAPER ===============
def get_reuters_trump_articles():
    print("üîç Fetching from Reuters...")
    url = "https://www.reuters.com/news/archive/politicsNews"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')

    articles = []
    seen = set()
    for a_tag in soup.find_all('a', href=True):
        headline = a_tag.get_text(strip=True)
        if "trump" in headline.lower():
            full_url = "https://www.reuters.com" + a_tag['href'].split('?')[0]
            if full_url in seen:
                continue
            seen.add(full_url)
            try:
                article = Article(full_url)
                article.download()
                article.parse()
                articles.append({
                    "source": "Reuters",
                    "title": article.title,
                    "text": article.text,
                    "url": full_url,
                    "publish_date": str(article.publish_date)
                })
            except:
                continue
    return articles

# =============== CNBC RSS FETCHER ===============
def get_cnbc_trump_articles():
    print("üîç Fetching from CNBC...")
    feed_url = "https://www.cnbc.com/id/10000113/device/rss/rss.html"  # Politics RSS
    feed = feedparser.parse(feed_url)

    articles = []
    for entry in feed.entries:
        if "trump" in entry.title.lower() or "trump" in entry.summary.lower():
            articles.append({
                "source": "CNBC",
                "title": entry.title,
                "summary": entry.summary,
                "url": entry.link,
                "published": entry.published
            })
    return articles

# =============== WSJ HEADLINE SCRAPER ===============
def get_wsj_headlines():
    print("üîç Fetching from WSJ (headlines only)...")
    url = "https://www.wsj.com/news/politics"
    headers = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')

    articles = []
    for h in soup.find_all(['h3', 'h2']):
        if h.text and "trump" in h.text.lower():
            articles.append({
                "source": "WSJ",
                "title": h.text.strip(),
                "summary": "Headline only",
                "url": "https://www.wsj.com/news/politics"
            })
    return articles

# =============== MERGE ALL ===============
def collect_all_trump_articles():
    reuters = get_reuters_trump_articles()
    cnbc = get_cnbc_trump_articles()
    wsj = get_wsj_headlines()

    all_articles = reuters + cnbc + wsj
    return all_articles

# =============== RUN AND SAVE ===============
articles = collect_all_trump_articles()

print(f" Found {len(articles)} Trump-related articles")

# Save to JSON
with open("trump_news.json", "w") as f:
    json.dump(articles, f, indent=2)

# Preview a few articles
for i, art in enumerate(articles[:3]):
    print(f"\n [{art['source']}] {art['title']}\n{art.get('url')}")

from google.colab import files
files.download("trump_news.json")

import spacy
import pycountry
from tqdm import tqdm

# Load spaCy model for NER
nlp = spacy.load("en_core_web_sm")

# Get list of ISO country names
COUNTRY_LIST = [country.name for country in pycountry.countries]

# Helper: Match named entities to countries
def extract_countries(text):
    doc = nlp(text)
    mentioned = set()
    for ent in doc.ents:
        if ent.label_ in ["GPE", "LOC"]:
            name = ent.text.strip()
            if name in COUNTRY_LIST:
                mentioned.add(name)
    return list(mentioned)

# Run on all collected articles
for article in tqdm(articles):
    full_text = article.get("text") or article.get("summary", "")
    article["countries"] = extract_countries(full_text)

# Preview result
for a in articles[:2]:
    print(f"\nüì∞ {a['title']}\nüåç Countries: {a['countries']}")


